After setting up the nodes as  in example [Link](https://github.com/thirumurthis/Learnings/blob/master/K8s/Kubernetes_cluster_notes.md).

##### To setup the kubectl command:
  - In case of the lab with 1 master 2 worker, just scp the admin.conf from the master /etc/kubernets/ to the host machine.
  
##### Once copied check/validate with below commands:
```
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:20:10Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"windows/amd64"}
Server Version: version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.3", GitCommit:"2e7996e3e2712684bc73f0dec0200d64eec7fe40", GitTreeState:"clean", BuildDate:"2020-05-20T12:43:34Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"}
```
```
$ kubectl version --short
Client Version: v1.17.0
Server Version: v1.18.3
```
```
$ kubectl cluster-info
Kubernetes master is running at https://172.42.42.100:6443
KubeDNS is running at https://172.42.42.100:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

#### First program to run in pod:

Create a yml file with nginx container (demo1.yml).
 
```yaml
apiVersion: v1

kind: Pod
metadata:
   name: myapp-pod
   labels:
     app: myapp
     type: testpod

spec:
    containers:
     - name: nginx-container
       image: nginx
```

##### To create pod using the `kubectl` command
```
$ kubectl create -f demo1.yml
pod/myapp-pod created
```

##### To check the status of deployed pod

- The status of the pod 'Pending` or `ContainerCreating`
```
C:\thiru\learn\k8s\certs\prog1>kubectl get pods
NAME        READY   STATUS              RESTARTS   AGE
myapp-pod   0/1     ContainerCreating   0          8s
```
Note: `READY` states that the Number of running container/ total number of container.

- The status of the pod after created `Running` 
```
C:\thiru\learn\k8s\certs\prog1>kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          40s
```

##### Creating an pod directly without yml file (`imperative way`)
```
$ kubectl run nginix --image=ngnix
```

##### To get the information of the pods running on which node.
```
$ kubectl get pods -o wide
```

##### To get the images used in the pod
```
$ kubectl describe pod <pod-name>
```

##### Given the pod edit it with necessary value.
```
$ kubectl edit pod <pod-name>
## this command opens up the vi with the pod info that was generated by the Kuberentes internally.
```

##### `kubectl apply`
```
$ kubectl apply -f <pod-descriptor file>
```

##### Comand to get the yaml file for a given pod
```
$ kubectl get pod <pod-name> -o yaml  > file-to-redirect.yml
```

#### Managing Labels:
##### After the pod is running how to view the label name.

The labels are already added in the Pod manifest yaml file (firstapp.yml).
```
 $ kubectl get pod myapp-pod --show-labels
 NAME        READY   STATUS    RESTARTS   AGE   LABELS
 myapp-pod   1/1     Running   0          46s   app=myapp,type=testpod
```
 
 ##### `Adding a label to a pod` running in the node.
```
 C:\thiru\learn\k8s\certs\prog1>kubectl label pod myapp-pod env=demo1
 pod/myapp-pod labeled

 C:\thiru\learn\k8s\certs\prog1>kubectl get pod myapp-pod --show-labels
 NAME        READY   STATUS    RESTARTS   AGE     LABELS
 myapp-pod   1/1     Running   0          5m54s   app=myapp,env=demo1,type=testpod
```
 
##### `Removing label to a pod` running in the node.
```
 ## Note the - sign at the end
 C:\thiru\learn\k8s\certs\prog1>kubectl label pod myapp-pod env-
 pod/myapp-pod labeled

  C:\thiru\learn\k8s\certs\prog1>kubectl get pod myapp-pod --show-labels
  NAME        READY   STATUS    RESTARTS   AGE     LABELS
  myapp-pod   1/1     Running   0          7m36s   app=myapp,type=testpod
```
 
##### `Overwrite/update the label to a pod` running in the node.
  
Add a label first as env=dev-demo1 and then update that to env=dev1
```
C:\thiru\learn\k8s\certs\prog1>kubectl get pod myapp-pod --show-labels
NAME        READY   STATUS    RESTARTS   AGE   LABELS
myapp-pod   1/1     Running   0          10m   app=myapp,env=dev-demo1,type=testpod

C:\thiru\learn\k8s\certs\prog1>kubectl label --overwrite pods myapp-pod env=dev1
pod/myapp-pod labeled

C:\thiru\learn\k8s\certs\prog1>kubectl get pod myapp-pod --show-labels
NAME        READY   STATUS    RESTARTS   AGE   LABELS
myapp-pod   1/1     Running   0          11m   app=myapp,env=dev1,type=testpod
```

##### Another representation of label overwrite command
```
## note the pod/<pod-name> representation
$ kubectl label pod/myapp-pod env=demo --over-write
```

##### `Search` using labels.
 - When manifest with many pods defined with labels and those are running.
 
```
## Say we need to know the list of pods that are having the label env=demo

$ kubectl get pods --selector env=demo

## To display the labels of the selector

$ kubectl get pods --selector env=demo --show-labels
```

##### How to apply multiple labels in the selector, when searching for pods
```
# the below will find the pods that contains the labels as stated with comma separated value.

$ kubectl get pods --selector env=demo,app=myapp
```

##### Applying `!=` search in the selector
```
# note the != in the selector

$ kubectl get pods --selector env!=demo,tier=front-end
```

##### `--selector` has a short form `-l`, lets use this in searching with set operator using `in`
```
## use " quotes when using from windows, since using ' reports exception "name cannot be provided for selector"
$ kubectl get pods -l "version in (1.0,2.0)"
```

##### using `notin` within selector
```
$ kubectl get pods -l "version notin (1.0,2.0)"
```

##### Delete pods using label selector 
```
## below will delete all the pods that has the matchin label env=demo
## note the resource name pods in this case. (it can be deployment, services, etc.)
 $ kubectl delete pods -l env=demo
```

##### `Delete all pods`
```
$ kubectl delete pods --all
```
Note: If the pods where created using deployment delte the deployment to delete the pods, since deployment will try to replicates the nodes as per the defintion files. (replicaSet). Deleting the deployment will delete the pod.

#### How to investigate what went wrong in the pod or deployment, using `describe`: 
```
## under the event section should be able to see the log message
## container info will be available over here too ( -o wide)
$ kubectl describe pod <pod-name>
```
### `rollout`
  - Recreate statergy 
     - all the previous version of pods will be scale down to 0 and new pods will be deployed.
  - Rolling update statergy (default one in k8s)
     - Will ensure that rolling update happens, meaning scale down and scale up happens gradually. one pod in previous version terminates and new pod gets created.
  
##### How to upgrade the deployment?
  - Say we have deployed a application with image version 1.0, using `$ kubectl create -f <deployment.yaml>` file.
  - If we have the image updated with the new version, we simply set the image to the deployment, like below:
  ```
  $ kubectl set image deployment/<previous-deployment-name> <new-image-version-name>
  ```
  - After setting the image, if we see the deployment, the new deployment is pdated.
  - At this moment, if you see the replicasets `$ kubectl get rs` should see two set of pods.
       - where there are two unique guid attached, and one is at the desired state with 1 or more replica set.
       
NOTE:
  - if the create command is started with `--record` option (will record)
  - after setting the latest version of image
  - using the command `$ kubectl rollout history deployment/<deployment-name>` will display the changes happened on the deployment, the revision happened.
  
##### How to `rollout` to the previous version  
```
$ kubectl rollout undo deployment/<deployment-name>
```

##### if we need to rollout to a specific version, then use
```
$ kubectl rollout undo deployment/<deployment-name> --to-revision=<version-number>
```

Demo:
 - Create a deployment for nginx:1.16 version impertively
 ```
 $ kubectl create deployment nginx --image=nginx:1.16
 ```
 
 - Check the `status of the rollout`, whenever a deployment happens, k8s will create a rollout.
 ```
 $ kubectl rollout status deployment nginx
 or
 $ kubectl rollout status deployment/nginx
 ```
 
 - Check the `rollout history` using below command, not that the output will have `<none>` in change-cause. (If we need to change-cause, we can use ` --record` option 
  ```
  $ kubectl rollout history deployment nginx
 ```
    - To view the deployment revision details for specific revision use below,
      ```
      $ kubectl rollout history deployment/nginx --revision=1
      ```
      
  - Update nginx to new version, deployment using command
  ```
  $ kubectl set image deployment nginx nginx=nginx:1.17 --record=true
  ```
      - Use `kubectl rollout history deployment/nginx` to see the change-cause
  
  - Update nginx to latest version, deployment using `kubectl edit`
    ```
    $ kubectl edit deployment/nginx
    ## edit the image version to latest
    
    ####check the rollout history for details
    $ kubectl rollout history deployment nginx --revision=3
    ```
  
##### How to undo changes, in the deployment.
  ```
  $ kubectl rollout undo deployment nginx
  
### check the history of the deployment rollout, notice the latest nginx is not available.
  ```
    
### Troubleshooting the pods using `logs`, `describe` or `exec`:
```
## from deployment perspective
$ kubectl describe deployment <deployment-name>
```

```
## pod name is obtained using kubectl get pod command
$ kubectl describe po/<pod-name>

or 

$ kubectl describe pod <pod-name>

## check the Events section which has the messages and info
```

#### Troubleshooting using the `logs` files:
```
## get the deployments info using kubectl get deployments.
## get the associated pods for that specific deployments. <pod-name>

$ kubectl logs <pod-name>
## the above command will return logs info of the pod
```

#### Trobuleshooting inside the pod itself using `exec` command:
```
## below command will take to the pod itself.
$ kubectl exec -it <pod-name> /bin/bash
root@pod-name:/#
```
##### Since we have multiple containers with the pod, in that case we need to exec to that container use below command `-c <container-name>`
  - Mostly we don't use -c since we execute single container in one pod.
  - but in case if we use multipe container, we can use below command.
```
$ kubectl exec -it <pod-name> -c <pod-name-without-uid> /bin/bash
```

### `ReplicationController` This is old way of implementing, new way is `replicaSet`.
 - Sample yaml manifest file for replication controller, this makes sure the number of pods running.
 
```yaml
# rcontroller-demo.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: app-rc
  labels:
     app: app-demo
     type: ngnix-demo
spec:
  template:
     metadata:
       name: app-demo
       labels:
         app: app-demo
         type: nginx-demo
     spec:
       containers:
       - name: nginx-containers
         image: nginx
  replicas: 3
```
 - Deployment and List the repilcation controller.
```
$ kubectl create -f rcontroller.yaml

$ kubectl get replicationcontroller

$ kubectl get pods
```

### `replicaSets` 
  - Difference between the ReplicationController and ReplicaSets is ReplicaSets has `selector` section where this is not mandatory in Replication controller.
  - ReplicaSet considers other pods which are already started into consideration.

```yaml
#This is different from the replication
#controller only this version is supporting replicaset
apiVersion: apps/v1 
kind: ReplicaSet
metadata:
  name: app-replicaset
  labels:
     app: myapp
     type: frontend
spec:
  template:
     metadata:
        name: myapp-pod
        labels:
           app: myapp
           type: frontend
     spec:
        containers:
        - name: nginx-containers
          image: nginx
  replicas: 3
  selector:    # This specifies which pods to be replicated 
      matchLabels:
         type: frontend
```

#### How to scale the replicaset values?
 - Different ways are there 
   - 1. using `kubectl replace`, in this case edit the yaml file, replicaset: 6 and issue below command
   
   ` $ kubectl replace -f replicaset-demo1.yaml`
   
   - 2. using `kubectl scale`, in this case the replica set value will not be updated in the yaml file.
       - when using the yaml manifest file
       `$ kubectl scale --replicas=6 -f replicaset-demo1.yaml`
       - when wanted to use the replicaset name `kubectl get replicaset`
       `$ kubetl scale --replicas=6 replicaset app-replicaset`
       or `$ kubectl scale --replicas=6 replocaset/app-replicaset`
       
    - 3. Auto scaling based on the resources, that is based on the load and resource usage.
    
sample output:
```
>kubectl get pods -l type=frontend
NAME                   READY   STATUS    RESTARTS   AGE
app-replicaset-74jkq   1/1     Running   0          3m28s
app-replicaset-csl5m   1/1     Running   0          3m28s
app-replicaset-zl598   1/1     Running   0          2m16s

>kubectl get rs
NAME             DESIRED   CURRENT   READY   AGE
app-replicaset   3         3         3       3m33s

>kubectl scale --replicas=4 replicaset/app-replicaset
replicaset.apps/app-replicaset scaled

>kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
app-replicaset-74jkq   1/1     Running   0          7m34s
app-replicaset-csl5m   1/1     Running   0          7m34s
app-replicaset-pgppm   1/1     Running   0          65s
app-replicaset-zl598   1/1     Running   0          6m22s
```

### `Deployments`
  - Deployments are used in case of production deployments of pods.

yaml file has the four basic properties, even in case of deployment.
```
apiVersion
kind
metadata
spec
```

The yaml file is similar to the above replicaset only change is the kind: Deployment
  
```yaml
# deployment-demo.yaml
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: app-deployment
  labels:
     app: myapp
     type: frontend
spec:
  template:
     metadata:
        name: myapp-pod
        labels:
           app: myapp
           type: frontend
     spec:
        containers:
        - name: nginx-containers
          image: nginx
  replicas: 3
  selector:    # This specifies which pods to be replicated 
      matchLabels:
         type: frontend
```
##### Docker ENTRYPOINT, CMD on docker file with that of the Manifest command and args 
 - ENTRYPOINT in docker is where the command is provided
 - CMD in docker is where the default arguments set.
 
 In Kubernetes, the ENTRYPOINT value is overrided by the command, and the CMD is overrided by args.
 
 - Sample manifest file for passing commands from manifest
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: command-demo
  labels:
    purpose: demonstrate-command
spec:
  containers:
  - name: command-demo-container
    image: ubuntu
    command: 
    - "sleep"
    - "1200"    
  restartPolicy: OnFailure
```
- using args to pass the value
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: command-demo
  labels:
    purpose: demonstrate-command
spec:
  containers:
  - name: command-demo-container
    image: ubuntu
    command: ["sleep"]  # this is the command 
    args: ["1200"]  # args passed to sleep 
  restartPolicy: OnFailure
 ```
----

#### Creating pods and generating pods using `imperative` command (not using the manifest file)

##### Create an ngnix pod
```
## to verify the command execution
$ kubectl run --generator=run-pod/v1 nginx-pod --image=ngnix:alpine --dry-run

## to print the output as yaml file
$ kubectl run --generator=run-pod/v1 nginx-pod --image=ngnix:alpine -o yaml

## to create the pod
$ kubectl run --generator=run-pod/v1 nginx-pod --image=ngnix:alpine 
```

##### Create an redis pod, with label type:backend
```
## we can use --dry-run or -o yaml/json/name
$ kubectl run --generator=run-pod/v1 redis-pod --image=redis:alpine --labels=type=backend
```

#### Create a service for the redis pod and expose the port with label type=backend-service
 type ClusterIP, so that the port is exposed within the cluster.
```
$ kubectl expose pod/redis-pod --port=6379 --name=redis-svc --labels=type=backend-service --dry-run

## Execution
$ kubectl expose pod/redis-pod --port=6379 --name=redis-svc --labels=type=backend-service


## This approach has no option to provide label, this will expect app=redis-pod as selector
$ kubectl create service clusterip redis-pod --tcp=6379:6379
```

#### Expose using the type:NodePort, expose port outside cluster
```
$ kubectl expose pod/redis-pod --port=6379 --name=redis-svc --type=NodePort
```

##### Exposing and image from docker and exposing the service:
```
## below creates deployment and pod (uses python flask to create web application)
$ kubectl run webapp-color --image=kodekloud/webapp-color 

## kubectl expose pod for 8080 port (make sure the application exposes uses that port, check docker file)

$ kubectl expose pod/<webapp-color-podname> --name webapp-color-service --port=8080 --type=NodePort --dry-run
### After service is created

$ kubectl get svc -o wide 
## check the pod hostname in my case it was kworker1.example.com and the namespace was demo.

## port name can be found using the kubectl get svc command.
> curl -k http://kworker1.example.com:31277 
### displayed the web page contents.
```

Dockerfile from hub.
```
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
```

Manifest pod file for the above docker refer the containerPort: (save as webapp-pod.yaml)
```
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  containers:
  - image: kodekloud/webapp-color
    name: webapp
    ports:
      - name: webapp
        containerPort: 8080
```
- use the above manifest file `$ kubectl create -f webapp-pod.yaml`

#### Create a deployment and scale the pods to 5 replicas
```
$ kubectl create deployment --image=redis:alpine redis-backend

### note there is no dry run option for the scale command.
$ kubectl scale deployment/redis-backend --replicas=5
```

----
### `readinessProbe` 
  - Used to know when the container is ready to take up the traffic.
  - Without the Pod status will be running, though the application is still not available for serving the traffic.
  - Readiness probe performs a test, in the below we have httpGet, and within the app we can se the path: /get/ready -> if the respone recived the status of the pod will be running.
  - By default, if the application is not ready by three attempts the probe will stop. we can use `failureThreshold` option in the readiness probe.
```yaml 
    readinessProbe:
          initialDelaySeconds: 10
          timeoutSeconds: 1
          # Probe for http
          httpGet:
            # Path to probe
            path: /
            # Port to probe
            port: 80
```
Other differen probes that can be used:
- TCP socket
 ```yaml
  # within the pod defintion file use the readiness like below
  readinessProbe:
     tcpSocket:
        port: 8080
 ```
 - Exec command
 ```
  readinessProbe:
     exec:
       command:
       - cat
       - /opt/app_readiness.txt
 ```
### `livenessProbe`
```yaml
        livenessProbe:
          initialDelaySeconds: 10
          timeoutSeconds: 1
          # Probe for http
          httpGet:
            # Path to probe
            path: /
            # Port to probe
            port: 80
```
 - similar to readinessProbe, livenessProbe also has TCPScoket (for database scenario) and exec probes (based on command ensuer the pods are healthy).
 
With the `livenessProbe`, the Pods and Deployment will be in running state, but notice the RESTARTS of the pods. In this case the pods will be restarted often.
  - At some point the pod will land up or goes to __`CrashLoopBackOff`__ status
---- 
### `namespaces` 
 - To run a pod in a name space, use --namespace or -n option.
 ```
 
 # using command
 $ kubectl run --generator=run-pod/v1 busybox --image=busybox -n dev --dry-run
 
 ## if wanted to create pod in a name space that is not having a namespace described
 ## dev is the namespace
 $ kubectl create -f <manifest file yaml> -n dev
 
 ## to use within the manifest file, under metadata properties of yaml include namespace: <value for namespace>
 $ kubectl create -f <manifest yaml file>
 ```

##### To access the service from one namespace to another?

```
## to access the service from antoher namespace
<service-name>.<namespace-name>.svc.cluster.local

svc => resources
cluster.local => is the domain
```

For a service running in a name space dev and needs to be accessed from test namespace:
```
# list the services running using 
$ kubectl get svc --all-namespaces -o wide

# pick the service to access from another namespace (from dev to access a test service)
$ redis-service.test.svc.cluster.local

# to access a service within the same name space just use the service name.
```
##### `default` is the K8s has a default namespace comes when installed or setup in cluster.

#### To change the namespace within the current context where the kubectl command executes
```
## create namespace
$ kubectl create namespace demo

$ kubectl get namespace

## to change the default or existing namespace to different namespace.
## previous was default in this case
$ kubectl config --current-context --namespace=demo

## to get the current context information 
$ kubectl config view
# check the namspace in the output

## to change back to the default namespace
$ kubectl config set-context --current --namespace=default

Or 

$ kubectl config set-context $(kubectl config current-context) --namespace=demo
```

#### Creating namespace using the manifest file
```yaml

apiVersion: v1
kind: Namespace
metadata:
  name: demo
# save the mainfest file and run the kubectl create -f <file.yaml>  
```

 - To list all the pods within all namespace
 ```
  ## note the namespaces (s at the end)
 $ kubectl get pods --all-namespaces
 ```
 
### `ResourceQuota` 
 - Used to limit the number of resources within the namespace
 
 Creating a ResourceQuota using manifest file:
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resourcequota-demo
  namespace: demo
  labels:
     type: compute-quota
spec:
  hard:
   requests.cpu: "2"
   requests.memory: 1Gi
   limits.cpu: "5"
   limits.memory: 5Gi

# use kubectl create -f <manifest yaml file>
```
----

### `ConfigMaps`
  - Application needs a way to pass data to them that can be changed at deploy time.
  - This is where the `configMaps` are used in Kubernetes.
    - For example a log level like (debug, error, etc) that needs to be passed at the start time.
    - Instead of hard coding this values we can pass these values as environment variables to the containers.
 
 ##### We have a deployment and we need to pass the log level as `env` variable, below is the example.
   - This can be acheived using the `configMap` as well
 ```yaml
 apiVersion: v1
 kind: Deployment
 metadata:
    name: loggerapp
 spec:
    replicas: 1
    template:
       metadata:
          labels:
            name: loggerapp
       spec:
         containers:
         - name: loggerapp
           image: thirumurthi/loggerapp:latest
           env:
           - name: log_level
             value: "info"
 ```
 
 #### Using `configMap` to change the environment variable at deployment time.
 - 1. Create the configMap using kubectl command, as below.
 ```
 ## we will be passing the log_level=info as environment variables during deployment.
 $ kubectl create configmap logger --from-literal=log_level=info
 ```
 - 2. How to refer the ConfigMap values from the deployment manifest.
 ```yaml
 apiVersion: extension/v1
 kind: Deployment
 metadata:
    name: loggerapp-configmap
 spec:
    replicas: 1
    template:
       metadata:
          labels:
            name: loggerapp-configmap
       spec:
         containers:
         - name: loggerapp-configmap
           image: thirumurthi/loggerapp:latest
           env:
           - name: log_level
             valueFrom:
               configMapKeyRef:
                  name: logger  # name of the configmap created 
                  key: log_level # read the key from the config map key
 ```
 
 - When we wanted to pass the __whole__ `configMap` to the pod. This is with the example of the python webapp (dockerfile reference)
   - Step 1: Create config map
     `$ kubectl create configmap --from-literal=APP_COLOR=green`
   - Step 2: Update the manifest file of pod: (note the use of envFrom not valueFrom)
      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: webapp
        labels:
          app: webapp
      spec:
        containers:
        - image: kodekloud/webapp-color
          name: webapp
          ports:
            - name: webapp
              containerPort: 8080
          envFrom:
          - configMapRef:
             name: webapp-config
      ```
 
 ##### To view the information about the `configMap`s:
 ```
 ## list the config map in for default namespace, for specific name space use -n
 $ kubectl get configmaps
 ```
 ```
 ## To list the contents of the configMap 
 $ kubectl get configmap/logger -o yaml
 ```
Note: 
 - Logs cannot be viewed on the deployments. 
 
### `secrets` in kubernetes:
  - Sensitive informaiton can be stored in Kubernetes as secrets.
  
##### how to create a secret:
 - Creating the secret
    ```
    ## below is the syntax to create a generic secret
    $ kubectl create secret generic <name-for-secret> --from-literal=somekey=somevalue

    $ kubectl create secret generic mytoken --from-literal=mytokenkey=123456
    ```
 - To view the secret info
    ```
    $ kubectl get secret <name-for-secret>
    $ kubectl get secret mytoken
    ```
    ```
    ## to display the contents 
    $ kubectl get secret mytoken -o yaml
    ## note the secret value will be encoded with base64 since we used generic
    ```
  
 - How to use the Kubernetes secrets within the deployment manifest. (`secretKeyRef`)
    ```yaml
    apiVersion: extensions/v1
    kind: Deployment
    metadata:
      name: secretreader
    spec:
      replicas: 1
      template:
        metadata:
          labels:
            name: secretreader
        spec:
          containers:
          - name: secretreader
            image: thirumurthi/secretapp:latest
            env:
            - name: mytoken
              valueFrom:
                 secretKeyRef:
                    name: mytoken  # name of the secret created
                    key: mytokenkey # the key used to set the value of the secret.
    ```
    
- To pass secret as an `environment` variable using `envFrom`

  - Step 1: create a secret.
  `$ kubectl create secret generic <secret-name> --from-literal=user=username --from-literal=password=paswd`
  
  - Step 2: view the secret info
  `$ kubectl get secret/<secret-name> -o yaml`
  
  - Step 3: Manifest file to pass the secret as environment values
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  containers:
  - image: kodekloud/webapp-color
    name: webapp
    ports:
      - name: webapp
        containerPort: 8080
    envFrom:
    - secretRef:
       name: <secret-name>
```

### `SecruityContext` 
  - Refer the Docker securtiy context.
  - Docker by default runs the process as root, this case be changed.
    - when executing the image use `user` arguments.
    - specify it when creating the image using `user` in Dockerfile

In Kubernetes, the security can be applied at Pod level or Container level.
    - If security is enabled at the pod level it will be applicable to all the containers within the pod.
    - The security can be set in the manifest file, refer the below pod manifest file.

Note: 
   When the security context is provided at the container level, that will take precedence even though it is mentioned at the pod level.
   
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntupod
spec:
# for pod level, specify secrutiy context at this level
  securitycontext:
     runAsUser: 1000
     capabilities:
        add: ["MAC_ADMIN","SYS_TIME"]
  containers:
  - image: ubuntu
    name: ubuntupod
    command: ["sleep","1200"]
    # for container level security specify at this level
    securitycontext:
       runAsUser: 1000
       capabilities:
          add: ["SYS_TIME"]
```

### `ServiceAccount`
  - In Kubernetes resource to resource communication happens using service account.
  - Authentication and Authorization.

##### How to create a serviceaccount?
```
## creation of service account
$ kubectl create serviceaccount testsa

> kubectl get serviceaccount
NAME      SECRETS   AGE
default   1         26h
testsa    1         6m28s

> kubectl describe serviceaccount testsa
Name:                testsa
Namespace:           demo
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   testsa-token-tqtxv
Tokens:              testsa-token-tqtxv
Events:              <none>

> kubectl describe secrets testsa-token-tqtxv
Name:         testsa-token-tqtxv
Namespace:    demo
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: testsa
              kubernetes.io/service-account.uid: aa9a70ac-6cb8-4474-8778-c0fad3d91c09

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  4 bytes
token:      eyJhbGciOiJS.......
```

- If third party application that needs to access say a the REST service exposed in the Kubernetes cluster,
  - Then a serviceaccount needs to be created.
  - This intern creates a secret to hold the token.
  - This token needs to be used as Bearer token when accessing the service from outside the cluster.

```
# when using curl command
$ curl -k <url-of-the-k8s-appp> --header "Authorization: Bearer eyJhGciOiJS..."
```

- In case if the application is within the Kubernets cluster, then the above step of creating and exporting the token is not required.
  - The serviceaccount can be created and monunted to the pod, so the application can use it.

In order to use the different service account on the pod manifest file to tell the pod to use serviceaccount other than default:
 ```yaml
 apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod-demo
  name: pod-demo
spec:
    containers:
    - image: busybox
    serviceAccount: testsa
 ```
Note about `serviceaccount`:
  - For every namespace a default service account is created automatically.
  - Whenever the pod is created the default serviceaccount and its token is automatically mounted to that pod as volume mount.
   `kubectl describe secret <secret-name>` check the volume mount.
  - the default serviceaccount has very limited access.
  - serviceaccount cannot edited on the pod using `kubect edit pod/<pod-name>`.
  - In case of deployment manifest, the service account can be edited.
 
 In order to not use the default service account, we can specify it in the manifest file using 
 ```
 # in the spec under container use the below if we don't want to mount the default serviceaccount.
 automountServiceAccountToken: false 
 ```
----
### Specifing the resource requirement within the pod manifest file.
```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod-demo
  name: pod-demo
spec:
    containers:
    - image: busybox
    resources:
      requests:
         memory: 1Gi
         cpu: 1
      limits:
         memory: 3Gi
         cpu:2 
```
- What is the cpu mean?
 - when specificing 1 it means 1 vCPU unit. (and the lowest unit would 1m, m - stands for milli)

- Memory representaton 1G and 1Gi is different in terms of memory.

The docker container has not limits to the resource consumption, which suffocating the native process. A limit can be set in the pod manifest file. using limit as added under requests in manifest file above.

If the container tries to allocate more memory repeatedly greater than the specified limit, the pod will be terminated.

 - In order to set the default value of these limit to a pod.
    - The `requests` and `limits` value should be set at the namespace.
    - below is applicable to default namespace.
 ```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 256Mi
    type: Container
 ```
### `Taints and Toleration`
  - Taints - are sort of restriction over the nodes in cluster that doesn't allow the pods to be set up by the scheduler in master.
  - Toleration - is adding some specification, so that specific pods can be added to the tainted nodes.
  
Note:
  - Taints are set on nodes
  - Toleration is set on the pods.
  
##### How to `taint` the node?

Specify the taint to the node using node name. It is a key value pair
```
$ kubectl taint nodes node-name key=value=taint-effect

## taint-effect values := NoScheduled | PrefereNoSchedule| NoExecute.
## taint-effect means what to do if taint is not tolerated.

## key=value; if we need the taint to be applied for the node where the pod has the app=blue, then it would be used

$ kubectl taint nodes kworker1.example.com app=blue:NoSchedule
```
Note:
  These pods might have scheduled before the taint was applied.

##### How to set `toleration` to the pod?
 - in the manifest or pod definiton files. 
 - under the containers level add `tolerations`
```yaml
#....
spec:
  containers:
    -image: nginx
  tolerations:
   - key: "app"
     operator: "Equal"
     value: "blue"
     effect: "Noschedule"
 # tolerations value are string so added in double quotes.   
 ```

Note:
 - There is no gaurantee that the tolerated pod will be executed in the Tainted node.
 
##### The master node in the cluster has a Taint set up by default so that not pods are scheduled, but this can be overriden, but the best practice is not to run any pod on the master.

 - To check the taint, ` $ kubectl describe node <master-node-name> | grep -i taint`
 
 ##### How to remove taints?
 ```
 $ kubectl taint node <node-name> key:value-
 ```

### `nodeselector` - Why is needed?
  - Say in a case if we have a 3 node cluster, out of which one node is more powerful in terms of CPU, Memory, etc.
  - We need the specfic workloads to be executed in specific node which is powerful node.
  - To achived this we can use `nodeSelector`
  - In order to perform this we need to define the node label in the pod defintion/manifest file. Note, the node should be set with the node before.
  
  Sample defintion or manifest file:
  ```yaml
  #...
  spec:
    containers:
      - image: ngnix
    nodeSelector:
      size: Large # the node label in this case, already set.
  ```
Note:
  - we are using a single label in this case.
  - we can't provide expression in the defintion yaml.
  
 ##### The nodeSelector is straight forward, assume a scenario where we need to run pod in highly powerful node and a moderately powerful node? Or Not to execute in specific set of nodes but on few other nodes in the cluster?
   - The answer to solve this is `Node Affinity` and Anti Affinity can be used.

### `Node Affinity`
  - To ensure that the pods are hosted in specific nodes.
 
 ```yaml
 # under the spec: at the level of container
 
 affinity:
   nodeAffinity:
     requiredDuringSchedulingIgnoredDuringException:
       nodeSelectorTerms:
         - matchExpressions:
           - key: size
             operator: In  # operators NotIn, Exists can be used
             values:  # label of the nodes
              - Large
              - Medium
 ```
Note: 
  - Exists operator can be used, this one will not check the values instead check to see if the key size exists in the labels on the nodes.
  
Different types of `Node Affinty`:
 - requiredDuringSchedulingIgnoredDuringExecution
 - preferredDuringSchedulingIgnoredDuringExecution
 - requiredDuringSchedulingRequiredDuringExecution (* - planned)

So when the labels are not avialbe during the pod execution, we use type to inform what action to be taken by the scheduler.

`requiredDuringSchedulingIgnoredDuringExecution` 
   - which means when scheduler couldn't identify matching nodes with the labels specified it will NOT schedule the pod.
   - In other case if the node has the matching label, this node will be used for running the pod.
   - This is to be used when the placing of pod in specific node is crucial.
   
`preferredDuringSchedulingIgnoreDuringExecution` 
   - which means when scheduler couldn't identify mathcing nodes with the labels specified it will schedule the pod on any available nodes simply ignoring the affinity. (In a way telling the scheduler to identify the best node for the workload)
   - This can be used when the pod placement is not important.
   
 `requiredDuringSchedulingRequiredDuringExecution`
   - This is the new option, where the change effecting affinity (like change in the label) after the pods are scheduled and running it will be impact the pod by evicting (or removed) the pod from the node.
  
  In both the case the part `IgnoredDuringExecution` means when the pod has been scheduled and running, if there are changes done that  affects the affinity for example say changing the label. 
   - In this secinario that changes to the label after the scheduler scheduled the pod on the node will not impact the running pod, which will be ignored.
   

----
### `jobs` in Kubernetes:
  - Jobs runs a pod once and then stop.
  - The output is kept around until it is deleted explicitly.
  - by default the jobs are ran in sequential way
  - using `parallelism` options, the jobs can be started parallely. This will make sure to start specified number of pods when started.
  - in order to run the job in multiple pods, we can use `completion` similar to replicaset.
  
Why we need Jobs?
 - The pod can do the same operation but the scheduler tries to restart the pods till the threshold is reached default `restartPloicy` is always. this can be set to Always.
 - For large process that can run parllelly, we can execute the process in n number of pods, and for some reason any of the pod has error, the job can manage to run the desired number of pod to process the jobs.
 - A job makes sure the set of pods running to achive the success status.

 - Deleting the job will delete the pods.

Sample job manifiest file
```yaml
## job-demo.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: countdownjob
spec:
  template:
     metadata:
        name: countdownjob
     spec:
        containers:
        - name: counter
          image: busybox
          command:
            - bin/sh
            - -c
            - "for i in 5 4 3 2 1 ; do echo $i; done"
         restartPolicy: Never    # other options, Always or onFailure
```
 - Like other manifest file we can use create command to deploy the jobs
 ```
 $ kubectl create -f job-demo.yaml
 ```
 
 ##### How to list the jobs and check the status?
 ```
 $ kubectl get jobs
 
 ## find the pods and check the logs
 ```
 
 ### `Cronjobs` in Kubernetes:
  - These are jobs which can run periodically like linux crontab.
 
 Sample cronjob manifest file:
 ```yaml
 ## cronjob-demo.yaml
 apiVersion: batch/v1
 kind: CronJob
 metadata:
   name: crondemo
 spec:
   schedule: "*/5 * * * *" # Crontab format runs every 5 mins 
   jobTemplate:
     spec:
       template:
          spec:
            containers:
            - name: crondemo
              image: busybox
              args:
              - /bin/sh
              - -c
              - date; echo Print from k8s
            restartPolicy: OnFailure  # other values Always or Never
    suspend: false  # true or false, if we don't wanted to be halted and 
                    # started at later point of time, the deployment will 
                    # be available not deleted. at jobtemplate level 
 ```
  - List the cronjobs using 
    ```
    $ kubectl get cronjob
    ```
  - List the jobs to see the cron triggered the jobs
    ```
    $ kubectl get job
    ```
  - Edit the cronjobs using below command in order to pause the job (set suspend to true)
    ```
    $ kubectl edit cronjob/crondemo
    ## opens up vi or editor configured edit and save the suspend to true.
    ```
  
 #### `Daemonset` in kubernets.
  - `DaemonSet` ensures that all nodes run a copy of a specific pod.
  - When a node is added to the cluster. pods are added to them as well.
  - Example: would be runing logging or monitoring agent on all of our nodes.
 
 Sample DaemonSet manifest file:
 ```yaml
 apiVersion: apps/v1
 kind: DaemonSet
 metadata:
   name: daemonset-demo
   namespace: default
   labels:
      app: daemonset-demo   
 spec:
   selector:
      matchLabels:
        name: daemonset-demo
   template:
      metadata:
        labels:
           name: daemonset-demo
      spec:
        containers:
        - name: daemonset-demo
          image: busybox
          args:
          - /bin/sh
          - -c
          - date; sleep 1000
          resources:
            limits:
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 500Mi
        terminationGracePeriodSeconds: 30  
```

  - Deploy the above daemonset manifest.
   ```
   $ kubectl create -f <manifest-file.com>
   ```
  - To list the Daemonset
  ```
  $ kubectl get daemonset
  ```
  
  ##### in order to make the daemonset to work on specific nodes use `nodeSelector`
  ```
  ## refer the yaml file above on how to add labels.
  ## if the nodes have labels it can be listed
  $ kubectl get nodes --show-labels
  ```
 - once the labels is added to the node 
 - in the yaml file we can add the nodeselctor.
   
```yaml
 apiVersion: apps/v1
 kind: DaemonSet
 metadata:
   name: daemonset-demo
   namespace: default
   labels:
      app: daemonset-demo   
 spec:
   selector:
      matchLabels:
        name: daemonset-demo
   template:
      metadata:
        labels:
           name: daemonset-demo
      spec:
        containers:
        - name: daemonset-demo
          image: busybox
          args:
          - /bin/sh
          - -c
          - date; sleep 1000
          resources:
            limits:
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 500Mi
        terminationGracePeriodSeconds: 30
        nodeSelector:
           env: "development"
```
  - Deploy the above yaml and list the daemon set to check the status.


### `StatefulSet`
 - statefulset manage deployment and scaling of the pods.
 - This guarantees abount the order and the uniqueness of these pods.
 - unlike deployment, statefulset manages the sticky identity for each pods.

To list the statefulset.
```
$ kubectl get statefulsets
```

#### `Ingress`:

Representation of Nginx Ingress controller

Representation:

![image](https://user-images.githubusercontent.com/6425536/84987118-7cc4c300-b0f4-11ea-96e0-e295bbdd4138.png)

### `Network Policy`
All the Pods can access other pods in the cluster accross different nodes.
This is basic requirement of K8s, but with network policy we can restrict this.
Setting up ingress and engress network policy we can tell one pod to allow traffic from another pod on specific port.

network policy is associated with the pod using 
yaml:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: network1-policy
spec:
   podSelector: # pod on which the n/w policy should be applied DB
      matchLabels:
        role: pod-name-selector
   policyTypes: # allow ingress/ engress traffic is specified here
   - Ingress:
   ingress:
   - from: # from which pod the ingress is allowed. API use selector
     - podSelector:
         matchLabels:
           role: pod-from-which-traffic-flow 
     ports:  # port that needs to be allwoed
     - protocol: TCP
       port: 3306
```

There are different providers of network solution available:
  Kube-router
  Calico
  Romana
  Weave-net
 
 Flannal - doesn't support the network policy check docs.
 
 Ingress/Egress traffic detail representation from the pod perspective:
 
 ![image](https://user-images.githubusercontent.com/6425536/85107974-b44e7080-b1c3-11ea-9bb6-1beea1abec0c.png)

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: example-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: <pod-on-which-the-policy-to-be-applied>
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: <backend-database-pod-which-recieves-egress-traffic>
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: <another-Pod-which-access-the-service>
    ports:
    - protocol: TCP
      port: 8080
```

### `volumes and persistence`

Sample manifest to use volumemount store to use the host directory. 

```yaml
apiVersion: v1
kind: Pod
metadata:
   name: persistence
   labels:
      type: volume
spec:
    containers:
    - image: alpine
      name: alpine
      command: ["/bin/sh","-c"]
      args: ["shuf -i 0-100 -n 1 >> /opt/random.out;"]
      volumeMounts:
      - mountPath: /opt  # directory of the container itself
        name: data-volume
    volumes:
    - name: data-volume
      hostPath:
        path: /data  # data folder should be available in the worker node host machine
        type: Directory
```

- Different types of volume available:
  - GlusterFS
  - NFS
  - Amazon webservice
  - Azure disk
  - Flocker
  - ceph
  - Scaleio
  - google

 Amazon webservice example
 ```yaml
 # ...
 volumes:
  - name: data-volume
    awsElasticBlockStore:
      volumeID: <volume-id-of-aws-store>
      fsType: ext4
# ...      
 ```

# persistence volume defintion
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pvolume1
spec:
  accessMode:
   - ReadWriteOnce # other type: ReadOnlyMany, ReadWriteMany
  capacity:
     storage: 1Gi
  hostPath:   # storage type Use NFS, Flocker, etc type
     path: /data
##### don't use hostPath type in production.

##### persistence volume claims to make storage availabe to node

PersistentVolume and PersistentVolumeclaim are two different object in the namespace.

Administrator creates the PersistentVolume
Developer/user creates the PersistentVolumeClaims to use the storage 

K8s binds the persistence voulme to persistence volume claim based on request and properties set on the volume.

Every persistent volume claim is bound to single peristent volume. During binding process K8s tries to find a persistent volume that has sufficitent capacity as requested by the claim and any other properties like access mode, volume modes, storage class.

If there are multiple matches for the persistent volume, still we can use labels and selectors to choose specific volumes.

A smaller claim can get bound to a larger vloume if that is a suitable match and there are no other option. There is 1-1 relation to the claims to the volume. So in this case the rest of the volume will not be used by another claim.

Creating a peristent volume claim:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-demo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
     requests:
        storage: 500Mi
```
##### refer the previously create volume defition, the storage is 1 Gi there.

```
$ kubectl get persistentvolumeclaims
```
##### What happens when deleting pvc?
When deleting the persistentvolumeclaim, the persistent volume does not get deleted since the default policy is retain.

In that case it needs to be done manually, but we can isntruct the claims to delete it automatically or recycle it.
This is because, if the claim is delted and the volume is not avialable for another claim until it is deleted/recycled and created.

The policy properties value: `persistentVolumeReclaimPolicy: Recycle` other options are `Retain`, `Delete`.

##### Where to use the persistent volume claim?
  - with the pod or deployment defintion yaml manifest file.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: pvc-demo
```
