After setting up the nodes as  in example [Link](https://github.com/thirumurthis/Learnings/blob/master/K8s/Kubernetes_cluster_notes.md).

##### To setup the kubectl command:
  - In case of the lab with 1 master 2 worker, just scp the admin.conf from the master /etc/kubernets/ to the host machine.
  
##### Once copied check/validate with below commands:
```
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:20:10Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"windows/amd64"}
Server Version: version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.3", GitCommit:"2e7996e3e2712684bc73f0dec0200d64eec7fe40", GitTreeState:"clean", BuildDate:"2020-05-20T12:43:34Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"}
```
```
$ kubectl version --short
Client Version: v1.17.0
Server Version: v1.18.3
```
```
$ kubectl cluster-info
Kubernetes master is running at https://172.42.42.100:6443
KubeDNS is running at https://172.42.42.100:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

#### First program to run in pod:

Create a yml file with nginx container (demo1.yml).
 
```yaml
apiVersion: v1

kind: Pod
metadata:
   name: myapp-pod
   labels:
     app: myapp
     type: testpod

spec:
    containers:
     - name: nginx-container
       image: nginx
```

##### To create pod using the `kubectl` command
```
$ kubectl create -f demo1.yml
pod/myapp-pod created
```

##### To check the status of deployed pod

- The status of the pod 'Pending` or `ContainerCreating`
```
C:\thiru\learn\k8s\certs\prog1>kubectl get pods
NAME        READY   STATUS              RESTARTS   AGE
myapp-pod   0/1     ContainerCreating   0          8s
```
Note: `READY` states that the Number of running container/ total number of container.

- The status of the pod after created `Running` 
```
C:\thiru\learn\k8s\certs\prog1>kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          40s
```

#### Creating an pod directly without yml file
```
$ kubectl run nginix --image=ngnix
```

#### To get the information of the pods running on which node.
```
$ kubectl get pods -o wide
```

#### To get the images used in the pod
```
$ kubectl describe pod <pod-name>
```

#### Given the pod edit it with necessary value.
```
$ kubectl edit pod <pod-name>
## this command opens up the vi with the pod info that was generated by the Kuberentes internally.
```

##### kubectl apply 
```
$ kubectl apply -f <pod-descriptor file>
```

#### Comand to get the yaml file for a given pod
```
$ kubectl get pod <pod-name> -o yaml  > file-to-redirect.yml
```

### Managing Labels:
##### After the pod is running how to view the label name.
  - The labels are already added in the Pod manifest yaml file (firstapp.yml).
 ```
 $ kubectl get pod myapp-pod --show-labels
 NAME        READY   STATUS    RESTARTS   AGE   LABELS
 myapp-pod   1/1     Running   0          46s   app=myapp,type=testpod
 ```
 
 ##### `Adding a label to a pod` running in the node.
 ```
 C:\thiru\learn\k8s\certs\prog1>kubectl label pod myapp-pod env=demo1
 pod/myapp-pod labeled

 C:\thiru\learn\k8s\certs\prog1>kubectl get pod myapp-pod --show-labels
 NAME        READY   STATUS    RESTARTS   AGE     LABELS
 myapp-pod   1/1     Running   0          5m54s   app=myapp,env=demo1,type=testpod
 ```
 
 ##### `Removing label to a pod` running in the node.
 ```
 ## Note the - sign at the end
 C:\thiru\learn\k8s\certs\prog1>kubectl label pod myapp-pod env-
 pod/myapp-pod labeled

  C:\thiru\learn\k8s\certs\prog1>kubectl get pod myapp-pod --show-labels
  NAME        READY   STATUS    RESTARTS   AGE     LABELS
  myapp-pod   1/1     Running   0          7m36s   app=myapp,type=testpod
 ```
 
 ##### `Overwrite/update the label to a pod` running in the node.
  - add a label first as env=dev-demo1 and then update that to env=dev1
 ```
C:\thiru\learn\k8s\certs\prog1>kubectl get pod myapp-pod --show-labels
NAME        READY   STATUS    RESTARTS   AGE   LABELS
myapp-pod   1/1     Running   0          10m   app=myapp,env=dev-demo1,type=testpod

C:\thiru\learn\k8s\certs\prog1>kubectl label --overwrite pods myapp-pod env=dev1
pod/myapp-pod labeled

C:\thiru\learn\k8s\certs\prog1>kubectl get pod myapp-pod --show-labels
NAME        READY   STATUS    RESTARTS   AGE   LABELS
myapp-pod   1/1     Running   0          11m   app=myapp,env=dev1,type=testpod
 ```

##### Another representation of label overwrite command
```
## note the pod/<pod-name> representation
$ kubectl label pod/myapp-pod env=demo --over-write
```

### `Searching` using labels.
 - When manifest with many pods defined with labels and those are running.
 
```
## Say we need to know the list of pods that are having the label env=demo

$ kubectl get pods --selector env=demo

## To display the labels of the selector

$ kubectl get pods --selector env=demo --show-labels

```

##### How to apply multiple labels in the selector, when searching for pods
```
# the below will find the pods that contains the labels as stated with comma separated value.
$ kubectl get pods --selector env=demo,app=myapp
```

##### Applying `!=` search in the selector
```
# note the != in the selector
$ kubectl get pods --selector env!=demo,tier=front-end
```

### `--selector` has a short form `-l`, lets use this in searching with set operator using `in`
```
## use " quotes when using from windows, since using ' reports exception "name cannot be provided for selector"
$ kubectl get pods -l "version in (1.0,2.0)"

```
##### using `notin` within selector
```
$ kubectl get pods -l "version notin (1.0,2.0)"
```

##### Performing delete for a group of lables 
```
## below will delete all the pods that has the matchin label env=demo
## note the resource name pods in this case. (it can be deployment, services, etc.)
 $ kubectl delete pods -l env=demo
```

##### `Delete all pods`
```
$ kubectl delete pods --all
```

Note: if there are some deployments associated to the pod, stop the deployment and then delete the pod.

##### `readinessProbe` 
  - Used to know when the container is ready to take up the traffic.
```yaml 
    readinessProbe:
          initialDelaySeconds: 10
          timeoutSeconds: 1
          # Probe for http
          httpGet:
            # Path to probe
            path: /
            # Port to probe
            port: 80
```

##### `livenessProbe`
```yaml
        livenessProbe:
          initialDelaySeconds: 10
          timeoutSeconds: 1
          # Probe for http
          httpGet:
            # Path to probe
            path: /
            # Port to probe
            port: 80
```

With the `livenessProbe`, the Pods and Deployment will be in running state, but notice the RESTARTS of the pods. In this case the pods will be restarted often.
  - At some point the pod will land up or goes to __`CrashLoopBackOff`__ status
 
 
#### How to investigate what went wrong in the pod or deployment, using `describe`: 
```
## under the event section should be able to see the log message
## container info will be available over here too ( -o wide)
$ kubectl describe pod <pod-name>
```

#### How to upgrade the deployment.
  - Say we have deployed a application with image version 1.0, using `$ kubectl create -f <deployment.yaml>` file.
  - If we have the image updated with the new version, we simply set the image to the deployment, like below:
  ```
  $ kubectl set image deployment/<previous-deployment-name> <new-image-version-name>
  ```
  - After setting the image, if we see the deployment, the new deployment is pdated.
  - At this moment, if you see the replicasets `$ kubectl get rs` should see two set of pods.
       - where there are two unique guid attached, and one is at the desired state with 1 or more replica set.
       
NOTE:
  - if the create command is started with `--record` option (will record)
  - after setting the latest version of image
  - using the command `$ kubectl rollout history deployment/<deployment-name>` will display the changes happened on the deployment, the revision happened.
  
##### How to `rollout` to the previous version  
```
$ kubectl rollout undo deployment/<deployment-name>
```

##### if we need to rollout to a specific version, then use
```
$ kubectl rollout undo deployment/<deployment-name> --to-revision=<version-number>
```

#### Troubleshooting the pods:
```
## from deployment perspective
$ kubectl describe deployment <deployment-name>
```

```
## pod name is obtained using kubectl get pod command
$ kubectl describe po/<pod-name>

or 

$ kubectl describe pod <pod-name>

## check the Events section which has the messages and info
```

#### Troubleshooting using the `logs` files:
```
## get the deployments info using kubectl get deployments.
## get the associated pods for that specific deployments. <pod-name>

$ kubectl logs <pod-name>
## the above command will return logs info of the pod
```

#### Trobuleshooting inside the pod itself using `exec` command:
```
## below command will take to the pod itself.
$ kubectl exec -it <pod-name> /bin/bash
root@pod-name:/#
```
##### Since we have multiple containers with the pod, in that case we need to exec to that container use below command `-c <container-name>`
  - Mostly we don't use -c since we execute single container in one pod.
  - but in case if we use multipe container, we can use below command.
```
$ kubectl exec -it <pod-name> -c <pod-name-without-uid> /bin/bash
```

### `ReplicationController` This is old way of implementing, new way is `replicaSet`.
 - Sample yaml manifest file for replication controller, this makes sure the number of pods running.
 
```yaml
# rcontroller-demo.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: app-rc
  labels:
     app: app-demo
     type: ngnix-demo
spec:
  template:
     metadata:
       name: app-demo
       labels:
         app: app-demo
         type: nginx-demo
     spec:
       containers:
       - name: nginx-containers
         image: nginx
  replicas: 3
```
 - Deployment and List the repilcation controller.
```
$ kubectl create -f rcontroller.yaml

$ kubectl get replicationcontroller

$ kubectl get pods
```

### `replicaSets` 
  - Difference between the ReplicationController and ReplicaSets is ReplicaSets has `selector` section where this is not mandatory in Replication controller.
  - ReplicaSet considers other pods which are already started into consideration.

```yaml
#This is different from the replication
#controller only this version is supporting replicaset
apiVersion: apps/v1 
kind: ReplicaSet
metadata:
  name: app-replicaset
  labels:
     app: myapp
     type: frontend
spec:
  template:
     metadata:
        name: myapp-pod
        labels:
           app: myapp
           type: frontend
     spec:
        containers:
        - name: nginx-containers
          image: nginx
  replicas: 3
  selector:    # This specifies which pods to be replicated 
      matchLabels:
         type: frontend
```

#### How to scale the replicaset values?
 - Different ways are there 
   - 1. using `kubectl replace`, in this case edit the yaml file, replicaset: 6 and issue below command
   
   ` $ kubectl replace -f replicaset-demo1.yaml`
   
   - 2. using `kubectl scale`, in this case the replica set value will not be updated in the yaml file.
       - when using the yaml manifest file
       `$ kubectl scale --replicas=6 -f replicaset-demo1.yaml`
       - when wanted to use the replicaset name `kubectl get replicaset`
       `$ kubetl scale --replicas=6 replicaset app-replicaset`
       or `$ kubectl scale --replicas=6 replocaset/app-replicaset`
       
    - 3. Auto scaling based on the resources, that is based on the load and resource usage.
    
sample output:
```
>kubectl get pods -l type=frontend
NAME                   READY   STATUS    RESTARTS   AGE
app-replicaset-74jkq   1/1     Running   0          3m28s
app-replicaset-csl5m   1/1     Running   0          3m28s
app-replicaset-zl598   1/1     Running   0          2m16s

>kubectl get rs
NAME             DESIRED   CURRENT   READY   AGE
app-replicaset   3         3         3       3m33s

>kubectl scale --replicas=4 replicaset/app-replicaset
replicaset.apps/app-replicaset scaled

>kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
app-replicaset-74jkq   1/1     Running   0          7m34s
app-replicaset-csl5m   1/1     Running   0          7m34s
app-replicaset-pgppm   1/1     Running   0          65s
app-replicaset-zl598   1/1     Running   0          6m22s
```

### `Deployments`
  - Deployments are used in case of production deployments of pods.

yaml file has the four basic properties, even in case of deployment.
```
apiVersion
kind
metadata
spec
```

The yaml file is similar to the above replicaset only change is the kind: Deployment
  
```yaml
# deployment-demo.yaml
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: app-deployment
  labels:
     app: myapp
     type: frontend
spec:
  template:
     metadata:
        name: myapp-pod
        labels:
           app: myapp
           type: frontend
     spec:
        containers:
        - name: nginx-containers
          image: nginx
  replicas: 3
  selector:    # This specifies which pods to be replicated 
      matchLabels:
         type: frontend
```
----

#### Creating pods and generating pods using imperative commnad

##### Create an ngnix pod
```
## to verify the command execution
$ kubectl run --generator=run-pod/v1 nginx-pod --image=ngnix:alpine --dry-run

## to print the output as yaml file
$ kubectl run --generator=run-pod/v1 nginx-pod --image=ngnix:alpine -o yaml

## to create the pod
$ kubectl run --generator=run-pod/v1 nginx-pod --image=ngnix:alpine 
```

##### Create an redis pod, with label type:backend
```
## we can use --dry-run or -o yaml/json/name
$ kubectl run --generator=run-pod/v1 redis-pod --image=redis:alpine --labels=type=backend
```

#### Create a service for the redis pod and expose the port with label type=backend-service
 type ClusterIP, so that the port is exposed within the cluster.
```
$ kubectl expose pod/redis-pod --port=6379 --name=redis-svc --labels=type=backend-service --dry-run

## Execution
$ kubectl expose pod/redis-pod --port=6379 --name=redis-svc --labels=type=backend-service


## This approach has no option to provide label, this will expect app=redis-pod as selector
$ kubectl create service clusterip redis-pod --tcp=6379:6379
```

#### Expose using the type:NodePort, expose port outside cluster
```
$ kubectl expose pod/redis-pod --port=6379 --name=redis-svc --type=NodePort
```

#### Create a deployment and scale the pods to 5 replicas
```
$ kubectl create deployment --image=redis:alpine redis-backend

### note there is no dry run option for the scale command.
$ kubectl scale deployment/redis-backend --replicas=5
```

### `namespaces` 
 - To run a pod in a name space, use --namespace or -n option.
 ```
 
 # using command
 $ kubectl run --generator=run-pod/v1 busybox --image=busybox -n dev --dry-run
 
 ## if wanted to create pod in a name space that is not having a namespace described
 ## dev is the namespace
 $ kubectl create -f <manifest file yaml> -n dev
 
 ## to use within the manifest file, under metadata properties of yaml include namespace: <value for namespace>
 $ kubectl create -f <manifest yaml file>
 ```

##### To access the service from one namespace to another?

```
## to access the service from antoher namespace
<service-name>.<namespace-name>.svc.cluster.local

svc => resources
cluster.local => is the domain

```

For a service running in a name space dev and needs to be accessed from test namespace:
```
# list the services running using 
$ kubectl get svc --all-namespaces -o wide

# pick the service to access from another namespace (from dev to access a test service)
$ redis-service.test.svc.cluster.local

# to access a service within the same name space just use the service name.
```
##### `default` is the K8s has a default namespace comes when installed or setup in cluster.

#### To change the namespace within the current context where the kubectl command executes
```
## create namespace
$ kubectl create namespace demo

$ kubectl get namespace

## to change the default or existing namespace to different namespace.
## previous was default in this case
$ kubectl config --current-context --namespace=demo

## to get the current context information 
$ kubectl config view
# check the namspace in the output

## to change back to the default namespace
$ kubectl config set-context --current --namespace=default

Or 

$ kubectl config set-context $(kubectl config current-context) --namespace=demo
```

#### Creating namespace using the manifest file
```yaml

apiVersion: v1
kind: Namespace
metadata:
  name: demo
# save the mainfest file and run the kubectl create -f <file.yaml>  
```

 - To list all the pods within all namespace
 ```
  ## note the namespaces (s at the end)
 $ kubectl get pods --all-namespaces
 ```
 
### `ResourceQuota` 
 - Used to limit the number of resources within the namespace
 
 Creating a ResourceQuota using manifest file:
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resourcequota-demo
  namespace: demo
  labels:
     type: compute-quota
spec:
  hard:
   requests.cpu: "2"
   requests.memory: 1Gi
   limits.cpu: "5"
   limits.memory: 5Gi

# use kubectl create -f <manifest yaml file>
```

#### Using items in the resourcequota manifest file with Kind `List`
```
apiVersion: v1
kind: List
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-high
  spec:
    hard:
      cpu: "1000"
      memory: 200Gi
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["high"]
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-medium
  spec:
    hard:
      cpu: "10"
      memory: 20Gi
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["medium"]
```
----

### `ConfigMaps`
  - Application needs a way to pass data to them that can be changed at deploy time.
  - This is where the `configMaps` are used in Kubernetes.
    - For example a log level like (debug, error, etc) that needs to be passed at the start time.
    - Instead of hard coding this values we can pass these values as environment variables to the containers.
 
 ##### We have a deployment and we need to pass the log level as `env` variable, below is the example.
   - This can be acheived using the `configMap` as well
 ```yaml
 apiVersion: extension/v1
 kind: Deployment
 metadata:
    name: loggerapp
 spec:
    replicas: 1
    template:
       metadata:
          labels:
            name: loggerapp
       spec:
         containers:
         - name: loggerapp
           image: thirumurthi/loggerapp:latest
           env:
           - name: log_level
             value: "info"
 ```
 
 #### Using `configMap` to change the environment variable at deployment time.
 - 1. Create the configMap using kubectl command, as below.
 ```
 ## we will be passing the log_level=info as environment variables during deployment.
 $ kubectl create configmap logger --from-literal=log_level=info
 ```
 - 2. How to refer the ConfigMap values from the deployment manifest.
 ```yaml
 
 apiVersion: extension/v1
 kind: Deployment
 metadata:
    name: loggerapp-configmap
 spec:
    replicas: 1
    template:
       metadata:
          labels:
            name: loggerapp-configmap
       spec:
         containers:
         - name: loggerapp-configmap
           image: thirumurthi/loggerapp:latest
           env:
           - name: log_level
             valueFrom:
               configMapKeyRef:
                  name: logger  # name of the configmap created 
                  key: log_level # read the key from the config map key
 ```
 
 ##### To view the information about the `configMap`s:
 ```
 ## list the config map in for default namespace, for specific name space use -n
 $ kubectl get configmaps
 ```
 ```
 ## To list the contents of the configMap 
 $ kubectl get configmap/logger -o yaml
 ```

Note: 
 - Logs cannot be viewed on the deployments. 
 
 ### `secrets` in kubernetes:
  - Sensitive informaiton can be stored in Kubernetes as secrets.
  
##### how to create a secret:
 - 1. Creating the secret
  ```
  ## below is the syntax to create a generic secret
  $ kubectl create secret generic <name-for-secret> --from-literal=somekey=somevalue
  
  $ kubectl create secret generic mytoken --from-literal=mytokenkey=123456
  ```
  -2. To view the secret info
  ```
  $ kubectl get secret <name-for-secret>
  $ kubectl get secret mytoken
  ```
  ```
  ## to display the contents 
  $ kubectl get secret mytoken -o yaml
  ## note the secret value will be encoded with base64 since we used generic
  ```
  
  3. How to use the Kubernetes secrets within the deployment manifest. (`secretKeyRef`)
  ```yaml
  apiVersion: extensions/v1
  kind: Deployment
  metadata:
    name: secretreader
  spec:
    replicas: 1
    template:
      metadata:
        labels:
          name: secretreader
      spec:
        containers:
        - name: secretreader
          image: thirumurthi/secretapp:latest
          env:
          - name: mytoken
            valueFrom:
               secretKeyRef:
                  name: mytoken  # name of the secret created
                  key: mytokenkey # the key used to set the value of the secret.
  ```

### `jobs` in Kubernetes:
  - Jobs runs a pod once and then stop.
  - The output is kept around until it is deleted explicitly.

Sample job manifiest file
```yaml
## job-demo.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: countdownjob
spec:
  template:
     metadata:
        name: countdownjob
     spec:
        containers:
        - name: counter
          image: busybox
          command:
            - bin/sh
            - -c
            - "for i in 5 4 3 2 1 ; do echo $i; done"
         restartPolicy: Never    # other options, Always or onFailure
```
 - Like other manifest file we can use create command to deploy the jobs
 ```
 $ kubectl create -f job-demo.yaml
 ```
 
 ##### How to list the jobs and check the status?
 ```
 $ kubectl get jobs
 
 ## find the pods and check the logs
 ```
 
 ### `Cronjobs` in Kubernetes:
  - These are jobs which can run periodically like linux crontab.
 
 Sample cronjob manifest file:
 ```yaml
 ## cronjob-demo.yaml
 apiVersion: batch/v1
 kind: CronJob
 metadata:
   name: crondemo
 spec:
   schedule: "*/5 * * * *" # Crontab format runs every 5 mins 
   jobTemplate:
     spec:
       template:
          spec:
            containers:
            - name: crondemo
              image: busybox
              args:
              - /bin/sh
              - -c
              - date; echo Print from k8s
            restartPolicy: OnFailure  # other values Always or Never
    suspend: false  # true or false, if we don't wanted to be halted and 
                    # started at later point of time, the deployment will 
                    # be available not deleted. at jobtemplate level 
 ```
    - List the cronjobs using 
    ```
    $ kubectl get cronjob
    ```
    - List the jobs to see the cron triggered the jobs
    ```
    $ kubectl get job
    ```
    - Edit the cronjobs using below command in order to pause the job (set suspend to true)
    ```
    $ kubectl edit cronjob/crondemo
    ## opens up vi or editor configured edit and save the suspend to true.
    ```
  
 #### `Daemonset` in kubernets.
   - `DaemonSet` ensures that all nodes run a copy of a specific pod.
   - When a node is added to the cluster. pods are added to them as well.
    - Example: would be runing logging or monitoring agent on all of our nodes.
 
 Sample DaemonSet manifest file:
 ```yaml
 apiVersion: apps/v1
 kind: DaemonSet
 metadata:
   name: daemonset-demo
   namespace: default
   labels:
      app: daemonset-demo   
 spec:
   selector:
      matchLabels:
        name: daemonset-demo
   template:
      metadata:
        labels:
           name: daemonset-demo
      spec:
        containers:
        - name: daemonset-demo
          image: busybox
          args:
          - /bin/sh
          - -c
          - date; sleep 1000
          resources:
            limits:
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 500Mi
        terminationGracePeriodSeconds: 30  
```
  - Deploy the above daemonset manifest.
  - To list the Daemonset
  ```
  $ kubectl get daemonset
  ```
  
  ##### in order to make the daemonset to work on specific nodes use `nodeSelector`
  ```
  ## refer the yaml file above on how to add labels.
  ## if the nodes have labels it can be listed
  $ kubectl get nodes --show-labels
  ```
 - once the labels is added to the node 
 - in the yaml file we can add the nodeselctor.
   
```yaml
 apiVersion: apps/v1
 kind: DaemonSet
 metadata:
   name: daemonset-demo
   namespace: default
   labels:
      app: daemonset-demo   
 spec:
   selector:
      matchLabels:
        name: daemonset-demo
   template:
      metadata:
        labels:
           name: daemonset-demo
      spec:
        containers:
        - name: daemonset-demo
          image: busybox
          args:
          - /bin/sh
          - -c
          - date; sleep 1000
          resources:
            limits:
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 500Mi
        terminationGracePeriodSeconds: 30
        nodeSelector:
           env: "development"
```
  - Deploy the above yaml and list the daemon set to check the status.


### `StatefulSet`
 - statefulset manage deployment and scaling of the pods.
 - This guarantees abount the order and the uniqueness of these pods.
 - unlike deployment, statefulset manages the sticky identity for each pods.

To list the statefulset.
```
$ kubectl get statefulsets
```
